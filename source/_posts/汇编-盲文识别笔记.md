---
title: 汇编-盲文识别笔记
date: 2022-03-28 12:56:11
tags:
categories:
---



# 链接



[常用字](https://wenku.baidu.com/view/3d2bd02b453610661ed9f40a.html)

[树莓派文字转语音](https://shumeipai.nxez.com/2013/10/05/three-methods-developed-in-text-to-voice-services.html )

[盲文数据集](https://github.com/yeluo1994/DSBI)



# 盲文认知

Braille

[参考2](https://www.zhihu.com/question/343076201)

[盲文翻译](http://www.braille.org.cn:8080/braille-web/blinds/index.html?userId=0&username=)

[盲文翻译1](https://easytactix.shsminfo.com/tobrialle.html)

![image-20220303202506822](https://picgo-freejim.oss-cn-beijing.aliyuncs.com/to_upload/image-20220303202506822.png)

![image-20220303202530028](https://picgo-freejim.oss-cn-beijing.aliyuncs.com/to_upload/image-20220303202530028.png)

![image-20220303202545034](https://picgo-freejim.oss-cn-beijing.aliyuncs.com/to_upload/image-20220303202545034.png)



Ps:

`g,j` 、`k,q` 、`h,x` 、 `y` 和 `ia `共用

要（iao）

![image-20220414143524921](https://picgo-freejim.oss-cn-beijing.aliyuncs.com/image-20220414143524921.png)

小（xiao）

![image-20220414143641967](https://picgo-freejim.oss-cn-beijing.aliyuncs.com/image-20220414143641967.png)

假（jia）

![image-20220414143604995](https://picgo-freejim.oss-cn-beijing.aliyuncs.com/image-20220414143604995.png)

我（uo）

![image-20220414143732197](https://picgo-freejim.oss-cn-beijing.aliyuncs.com/image-20220414143732197.png)



以（i）

![image-20220414143754911](https://picgo-freejim.oss-cn-beijing.aliyuncs.com/image-20220414143754911.png)





有

![image-20220414163733630](https://picgo-freejim.oss-cn-beijing.aliyuncs.com/image-20220414163733630.png)



单 i 为 yi

单 uo 为 wo

单 iao 为 yao

单 zh 为 zhi

单iu 为 you



5/11 new：

单 u 为 wu



`g/k/h` 在韵母 `i/u/ü` 时，变读为`j/q/x`



`z/c/s/zh/ch/sh/r` 后面的 `i` 省略



word完成后就转换，不符合规定的直接踢掉



word的长度有限制的



## 写盲文

绿板

[写盲文简介](https://www.bilibili.com/video/BV1jq4y1579H/)







## 拼音

[声母](https://www.bilibili.com/video/BV1E64y1e7c8)

[拼音](https://www.bilibili.com/video/BV1Q54y1J7rQ)

`12点b`+`35点a` = ba 爸

![image-20220304094158294](https://picgo-freejim.oss-cn-beijing.aliyuncs.com/to_upload/image-20220304094158294.png)

上图表示

```
爸爸
```



## 数字

数字前面都带符号 - `3456点`

如：

数字1

`3456点`+`1点`

`235点`是加号

`236点`是乘

![image-20220304092844535](https://picgo-freejim.oss-cn-beijing.aliyuncs.com/to_upload/image-20220304092844535.png)

上图表示

```
数字1 + 数字1 - 数字1 x 数字1 ÷ 数字1
```



# 数据标记

![image-20220304095144998](https://picgo-freejim.oss-cn-beijing.aliyuncs.com/to_upload/image-20220304095144998.png)

![image-20220304094917965](https://picgo-freejim.oss-cn-beijing.aliyuncs.com/to_upload/image-20220304094917965.png)

```
bndbox区域被标记为build建筑
```









# TensorFlow



## demo

Ps：`Flatten()`不可少

```python
import tensorflow as tf

from tensorflow.keras import datasets, layers, models
import matplotlib.pyplot as plt


# get datasets
(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()

# Normalize pixel values to be between 0 and 1
train_images, test_images = train_images / 255.0, test_images / 255.0

# ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']

model = models.Sequential()

# 卷积层
# input_size = 32 卷及操作输出的维度（与步长有关）
# kernel_size = (3,3)
# activation 激活函数relu
# input_shape size = (32,32) channel = 3 - RGB, 1 - RGB
model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))

# 池化 - 压缩并保留信息
# pool_size = 默认是(2,2)
# stride 步长 默认是2
model.add(layers.MaxPooling2D((2, 2)))

model.add(layers.Conv2D(64, (3, 3), activation='relu'))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(64, (3, 3), activation='relu'))


# 多维化一维
model.add(layers.Flatten())
# 全连接层
# 维度对应与上面卷积操作输出的维度64应该一致
model.add(layers.Dense(64, activation='relu'))

# 维度对应10个labels
model.add(layers.Dense(10))

model.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])

history = model.fit(train_images, train_labels, epochs=10,
                    validation_data=(test_images, test_labels))

test_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)
```

## 使用经典神经网络

```python
from keras.applications.vgg16 import VGG16

input = keras.layers.Input((64, 64, 1))

self.model = VGG16(inputs=input, classes=labels_num)
self.model.compile(...)
```



### fit参数说明

[参考](https://blog.csdn.net/LuYi_WeiLin/article/details/88555813)

`class_weight` - 字典，将不同的类别映射为不同的权值，该参数用来在训练过程中调整损失函数（只能用于训练）

`validation_data` - 形式为（X，y）的tuple，是指定的验证集。此参数将覆盖validation_spilt

`batch_size` - 指定进行梯度下降时每个batch包含的样本数。训练时一个batch的样本会被计算一次梯度下降，使目标函数优化一步



## 激活函数

## softmax

Softmax函数将K维的实数向量压缩（映射）成另一个K维的实数向量

其中向量中的每个元素取值都介于 (0，1) 之间。常用于多分类问题





## sigmoid

Sigmoid 将一个实数映射到 (0,1) 的区间，可以用来做二分类




## 损失函数

loss策略影响很大的！

二元分类还是多分类

二分类，交叉熵

```python
model.add(layers.Dense(1, activation='sigmoid'))
model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])

```

本文介绍[Keras](https://so.csdn.net/so/search?q=Keras&spm=1001.2101.3001.7020)的几个常用的分类损失函数

包括categorical_crossentropy，sparse_categorical_crossentropy

binary_crossentropy，hinge, squared_hinge，categorical_hinge



### binary_crossentropy

![image-20220405160940823](https://picgo-freejim.oss-cn-beijing.aliyuncs.com/image-20220405160940823.png)



### categorical_crossentropy

![image-20220405161007503](https://picgo-freejim.oss-cn-beijing.aliyuncs.com/image-20220405161007503.png)





### sparse_categorical_crossentropy

稀疏









## logits

一个非空的Tensor。必须是下列类型之一：half， float32，float64



## Softmax

n维多分类问题





## validation_data

不可少







## 模型导入/导出

### 导入

```python
checkpoint_path = 'm1.h5'
model.save(checkpoint_path)
```





### 导出

```python
checkpoint_path = 'm1.h5'
model: models.Sequential
model = models.load_model(checkpoint_path)
model.summary()
```



### 代码提示问题

```
model: models.Sequential
```







## tensor

多维数组





## 数据集



### 训练集

这个是最好理解的，用来训练模型内参数的数据集

Classfier直接根据训练集来调整自身获得更好的分类效果



### 验证集

用于在训练过程中检验模型的状态，收敛情况

验证集通常用于调整超参数，根据几组模型验证集上的表现决定哪组超参数拥有最好的性能

同时验证集在训练过程中还可以用来监控模型是否发生过拟合，一般来说验证集表现稳定后

若继续训练，训练集表现还会继续上升，但是验证集会出现不升反降的情况

这样一般就发生了过拟合。所以验证集也用来判断何时停止训练



### 测试集

测试集用来评价模型泛化能力，即之前模型使用验证集确定了超参数，使用训练集调整了参数，最后使用一个从没有见过的数据集来判断这个模型是否Work。





### 数据增强

```
pip install Augmentor
```



```
import Augmentor
# 1. 指定图片所在目录
p = Augmentor.Pipeline("./imageSet")


# 2. 增强操作
# 旋转 概率0.7，向左最大旋转角度10，向右最大旋转角度10
p.rotate(probability=0.7,max_left_rotation=10, max_right_rotation=10)

# 放大 概率0.3，最小为1.1倍，最大为1.6倍；1不做变换
#Typical values may be min_factor=1.1 and max_factor=1.5
p.zoom(probability=0.3, min_factor=1.1, max_factor=1.6)


# resize 同一尺寸 200 x 200
p.resize(probability=1,height=200,width=200)

# value between 0.1 and 1.0, where 1 represents a tilt of 45 degrees
# 垂直方向形变
p.skew_tilt(probability=0.5, magnitude=1)

# 四边形形变
p.skew_corner(probability=0.5,magnitude=1)


# 3. 指定增强后图片数目总量
p.sample(3000)
```









## 图片数据结构

无论是几通道的，都是四维数组

通道的区别只是最低维内有几个元素



```
(65, 102, 4)
```

图片规格`65x102`

4通道

除了RGB

多一个参数表示透明度(Alpha)

叫做RGBA



3通道（4维数组）

```
(32,32,3)
```

```
[[[[0.23137255 0.24313725 0.24705882]
   [0.16862745 0.18039216 0.17647059]
   [0.19607843 0.18823529 0.16862745]
   ...
   [0.61960784 0.51764706 0.42352941]
   [0.59607843 0.49019608 0.4       ]
   [0.58039216 0.48627451 0.40392157]]]]
```





如果是1通道 - 黑白照片 - 灰度值不是(0,255)而是0或1

```
(32,32,1)
```

```python
[[[[1.]
   [1.]
   [1.]
   ...
   [1.]
   [1.]
   [1.]]]
# 一个像素点的值为0或1
```









### 训练集图片规格不一

`resize`没有返回值

`reshape` 





## 训练报错



```
Make sure all arrays contain the same number of samples.
```

解决：

图片的数组规格不一致





```
Failed to convert a NumPy array to a Tensor (Unsupported object type float).
```

解决：

```
images = np.array(images, dtype=object).astype('float32')
```



```

```

解决：

```
    history = model.fit(train_images, train_labels, epochs=1,
                        batch_size=len(train_images))
```









## label

不是具体的`cat`

而是数字

```
[[6]
 [9]
 [9]
 ...
 [9]
 [1]
 [1]]
```





## 过拟合问题

机器学习中最常见的问题

![image-20220308095953235](https://picgo-freejim.oss-cn-beijing.aliyuncs.com/to_upload/image-20220308095953235.png)





## 卷积层



### conv1D

卷积层

![image-20220329124839396](https://picgo-freejim.oss-cn-beijing.aliyuncs.com/image-20220329124839396.png)

把范围为k的卷积成一个值，完成卷积操作



### conv2D

![image-20220329125624396](https://picgo-freejim.oss-cn-beijing.aliyuncs.com/image-20220329125624396.png)



（k*k）的矩阵遍历图像，提取特征值，完成卷积操作



### conv3D

![image-20220329125718442](https://picgo-freejim.oss-cn-beijing.aliyuncs.com/image-20220329125718442.png)



看箭头的方向遍历立方体，完成卷积操作



### 2D Conv with 3D-inputs

卷积层

![](https://i.stack.imgur.com/FjvuN.gif)





## 训练策略

错误的答案的推测可能性很高 - 错误答案预测率高

相近的混淆

空白的，反而识别到某个字





测试集的准确率几乎没啥变化，都在0.78

训练集和验证集的也是，保持在0.9多 ？？？



### shuffle

```python
np.random.seed(200)
np.random.shuffle(images)
np.random.seed(200)
np.random.shuffle(labels)
```

kersa可以在`fit`处置`True`



### epochs



epochs -- 影响不大 - 一般是逼近于一种状态，不会有太多的波动

会不会导致过拟合？- 不好说



量变还真会引起质变





### 卷积核个数

对上述问题影响大

对于错误答案的猜测率明显有了下降

说明特征值的提取更加顺利 - 第一个卷积层的给大点





### dropout

添加dropout后，可以缓解

把错误的可能性降到0.5左右





### 方案

总共55个要识别的切片

我可不可以训练出6个模型！



### 卷积层数



增加了一层，然后全部都识别成了一个字，及其荒诞

增加一层参数变少

但是训练所需时间反而增加





### 卷积核大小

卷积核从(5,5)调整到(2,2)

泛化能力变到了0



缩小到（3，3），可以加快拟合速度





### 全连接层数



### 全连接层核心数

越多，参数也越多





### 池化层

多一层

参数少很多



### 图像处理

![image-20220402194519837](https://picgo-freejim.oss-cn-beijing.aliyuncs.com/image-20220402194519837.png)





### padding

![image-20220402194333365](https://picgo-freejim.oss-cn-beijing.aliyuncs.com/image-20220402194333365.png)



### 拟合情况

一段时间内准确率几乎不变

但是并不代表继续训练下去不会改变







### batchsize

一次放入n个样本进去训练

增大batchsize，可以减少一次epoch的迭代次数

可以有效提升拟合速度

但是泛化能力不一定

增大batchsize，拟合速度快，但是训练出来的模型效果差不多



高至1024 - 拟合速度反而变慢

准确率也不高

有没有可能这个准确率才是正确的。。



### 激活函数

`softmax`之前用`tanh`

原来是`relu`

真的可以有效帮助拟合



调试：

老是识别到声调 -- 把中间的一个激活函数relu改为了tanh，反而是老是识别不到声调

而且预测率也不高，约在0.5

往好的方向发展了



### optimizers

影响很大

可以有效优化

可以不改变cnn结构，试着调整optimizers

```
sgd
rmsprop
adam
adadelta
adagrad
adamax
nadam
ftrl
```







### dropout介绍

（确实很有效得提高了模型的泛化能力）

让某些神经元以一定的概率停止工作

阻止神经元的共适应

```python
self.model.add(layers.Dropout(0.4))
self.model.add(layers.Dense(429, activation='softmax'))
```

![image-20220328200040583](https://picgo-freejim.oss-cn-beijing.aliyuncs.com/image-20220328200040583.png)



dropout会使训练效果变差，因为每次训练是有些神经元莫名其妙就没了

但这个这是在训练集变差，可以避免过拟合



相当于总体分成很多样本（因为每次采集的神经元不一样，得到的网络结构不一样）

求平均值，这样结果就会更好

![image-20220329110326567](https://picgo-freejim.oss-cn-beijing.aliyuncs.com/image-20220329110326567.png)









### 图像大小

常见的图像大小包括64x64、128x128、28x28 (MNIST)和224x224 (vgg -16)。



从一个大分辨率的图像到一个小尺寸的图像，比如28x28，通常会导致大量的像素化，这往往会对你的模型的性能产生负面影响



### 训练次数

以+25、+100的间隔







### loss变化

train loss 不断下降，test loss不断下降，说明网络仍在学习;

train loss 不断下降，test loss趋于不变，说明网络过拟合;

train loss 趋于不变，test loss不断下降，说明数据集100%有问题;

train loss 趋于不变，test loss趋于不变，说明学习遇到瓶颈，需要减小学习率或批量数目;

train loss 不断上升，test loss不断上升，说明网络结构设计不当，训练超参数设置不当，数据集经过清洗等问题。



### 结构调整

(1) 添加更多的层。
(2) 让每一层变得更大。
(3) 训练更多的轮次。





### filters！！



第一层的filter, 数量不要太少. 否则根本学不出来(底层特征很重要).



每一个Filter的深度要和输入层深度保持一致



卷积核的数量



![image-20220329155237749](https://picgo-freejim.oss-cn-beijing.aliyuncs.com/image-20220329155237749.png)







### outputshape

(None, 62, 62, 64)

(batch_size, height, width, depth)





### Dense

map a spatial structure directly into a dense layer





### flatten

`Flatten` layers are used when you got a multidimensional output 

and you want to make it linear to pass it onto a `Dense` layer. 

If you are familiar with `numpy`, it is equivalent to `numpy.ravel`. 





### outputshape



### params





### batchsize

For instance, let's say you have 1050 training samples 

and you want to set up a `batch_size` equal to 100. 

The algorithm takes the first 100 samples (from 1st to 100th) from the training dataset and trains 

Next, it takes the second 100 samples (from 101st to 200th) and trains the network again. 

We can keep doing this procedure until we have propagated all samples through of the network. 

Problem might happen with the last set of samples. 

In our example, we've used 1050 which is not divisible by 100 without remainder. The simplest solution is 

just to get the final 50 samples and train the network.





batch_size 2 4 6 8 16 32 64 128 256

对上述问题的影响相对较小





### max pooling

taking maximum value over the whole data area

盲文这应该是要选择最大池化的



### avg pooling

取平均值



### 学习率

区分stride 步长

```python
Conv2D(256, (5, 5), strides=(1, 1), activation='relu')
```

这个是卷积操作时遍历得步长



![image-20220329134430815](https://picgo-freejim.oss-cn-beijing.aliyuncs.com/image-20220329134430815.png)





### 学习率调整

```python
reduce_lr = ReduceLROnPlateau(monitor='val_loss', patience=10, mode='auto')

self.model.fit(self.train_images,
                       self.train_labels,
                       epochs=35,
                       verbose=1,
                       batch_size=100,
                       callbacks=[reduce_lr],
                       validation_data=(self.valid_images, self.valid_labels))
```

上述表示每10epocs，如果效果没有优化，则调整学习率

一般是到后边要降低学习率

![image-20220329134024139](https://picgo-freejim.oss-cn-beijing.aliyuncs.com/image-20220329134024139.png)

到最优解那，如果学习率还大的话，会震荡

太小的话，收敛得又慢



所以最好的策略是，先大后小





### 激活函数





### 训练的本质

特征值的提取



### 泛化能力

训练时准确率高，但是不能准确识别数据集外的图像

那就是模型的泛化能力不足



如果欠拟合，就增大模型；如果过拟合，就添加数据或调整

用由粗到细随机搜索优化超参数





增加层数，参数增加，训练时间会增加





### 模型的选择

你的输入是图片就选择类似LeNet的架构，输入是语言序列就选择有一个隐藏层的LSTM。











## LeNet

原结构

```python

model = Sequential()
model.add(Conv2D(32,(5,5),strides=(1,1),input_shape=(28,28,1),padding='valid',activation='relu',kernel_initializer='uniform'))
model.add(MaxPooling2D(pool_size=(2,2)))
model.add(Conv2D(64,(5,5),strides=(1,1),padding='valid',activation='relu',kernel_initializer='uniform'))
model.add(MaxPooling2D(pool_size=(2,2)))
model.add(Flatten())
model.add(Dense(100,activation='relu'))
model.add(Dense(10,activation='softmax'))
model.compile(optimizer='sgd',loss='categorical_crossentropy',metrics=['accuracy'])
```











## 数据集优化

考虑到切片的盲文点都是在靠近边界的地方

所以，决定重构数据集





### 验证集的loss越来越大，准确率越来越低

调整网络的结构，准确率明显提高





# 模型训练策略



## 数据集

数据集尽可能大



Ps：

数据集数量不够可能都训练不到所有参数



## 激活函数



## 卷积层

前面的尽量保留特征值

不卷太多

后边再卷



## pretrain model

由于ImageNet数以百万计带标签的训练集数据

使得如CaffeNet之类的预训练的模型具有非常强大的泛化能力

这些预训练的模型的中间层包含非常多一般性的视觉元素

我们只需要对他的后几层进行微调

在应用到我们的数据上，通常就可以得到非常好的结果



# CNN



## 基本原理

首先了解传统神经网络 - 计算量庞大

![image-20220402194000001](https://picgo-freejim.oss-cn-beijing.aliyuncs.com/image-20220402194000001.png)

CNN应运而生

![image-20220402194051283](https://picgo-freejim.oss-cn-beijing.aliyuncs.com/image-20220402194051283.png)







主要区别：

局部感知区域、权值共享、多核卷积



### 权值共享

卷积核映射的区域，自然就叫做权值共享了







### 非线性变换（激活函数）

机器学习的核心

之前谈到的无论是全连接层还是卷积层的计算都是简单的乘加运算

也称之为线性运算，这样我们可以作为线性层



线性层的特征表达能力是有限的，所以在这些线性计算之后又引入了非线性计算

增强模型特征的表达能力，也就是大家熟知的激活层，也称为非线性层



![image-20220402200701254](https://picgo-freejim.oss-cn-beijing.aliyuncs.com/image-20220402200701254.png)



### 核心

深度学习的精髓是利用多层特征组合，将简单的特征组合成复杂的抽象特征









## 卷积

### 卷积核

![image-20220402195202514](https://picgo-freejim.oss-cn-beijing.aliyuncs.com/image-20220402195202514.png)



### 特征是什么









### 卷积的意义

（1）滤波

可以把一些尖锐点给去掉



## 池化

压缩并保留信息





### 池化size

默认是（2,2）





### 池化stride

步长

一般strides为2

滑动窗口在各个维度上移动的步数

![image-20220305120356558](https://picgo-freejim.oss-cn-beijing.aliyuncs.com/to_upload/image-20220305120356558.png)









## 全连接层

### 为什么叫全连接

就是由于每一个输出节点与每一个输入节点都有连接



![image-20220402195518712](https://picgo-freejim.oss-cn-beijing.aliyuncs.com/image-20220402195518712.png)



### 全连接层调整

![image-20220402195628386](https://picgo-freejim.oss-cn-beijing.aliyuncs.com/image-20220402195628386.png)





## 激活函数

![image-20220403092900249](https://picgo-freejim.oss-cn-beijing.aliyuncs.com/image-20220403092900249.png)



## batch normalization



对网络某一层A的输出数据做归一化

然后送入网络下一层B



不过不要每一层都进行标准化操作

在开头做就行

### relu

![image-20220403092934283](https://picgo-freejim.oss-cn-beijing.aliyuncs.com/image-20220403092934283.png)



## 梯度

梯度是一个向量，即有方向有大小

梯度的方向是最大方向导数的方向

梯度的值是最大方向导数的值

## 前向传播

前向传播比较简单，就是向量点乘

也就是加权求和，然后经过一个激活函数





## 反向传播算法

![image-20220411181318287](https://picgo-freejim.oss-cn-beijing.aliyuncs.com/image-20220411181318287.png)





## 神经网络结构

![image-20220403093142825](https://picgo-freejim.oss-cn-beijing.aliyuncs.com/image-20220403093142825.png)



## dropout

![image-20220403093309366](https://picgo-freejim.oss-cn-beijing.aliyuncs.com/image-20220403093309366.png)



![image-20220403093404190](https://picgo-freejim.oss-cn-beijing.aliyuncs.com/image-20220403093404190.png)

## 经典神经网络

[博客](https://blog.csdn.net/mubaba_/article/details/107092042)

[介绍](https://sh-tsang.medium.com/review-nasnet-neural-architecture-search-network-image-classification-23139ea0425d)



![image-20220403093619214](https://picgo-freejim.oss-cn-beijing.aliyuncs.com/image-20220403093619214.png)

### AlexNet

![image-20220403093509045](https://picgo-freejim.oss-cn-beijing.aliyuncs.com/image-20220403093509045.png)

![image-20220403093459070](https://picgo-freejim.oss-cn-beijing.aliyuncs.com/image-20220403093459070.png)





### ResNet









# 图像识别



## ResNet

![image-20220402193616484](https://picgo-freejim.oss-cn-beijing.aliyuncs.com/image-20220402193616484.png)

![image-20220402193715393](https://picgo-freejim.oss-cn-beijing.aliyuncs.com/image-20220402193715393.png)



# 目标定位



## 语义分割

semantic segmentation

同一类的物体为同一颜色

![image-20220308142850440](https://picgo-freejim.oss-cn-beijing.aliyuncs.com/to_upload/image-20220308142850440.png)



## 实例分割(盲文)

instance segmentation

各一个物体示例为一个单位

![image-20220308142903868](https://picgo-freejim.oss-cn-beijing.aliyuncs.com/to_upload/image-20220308142903868.png)

Mask R-CNN



## 全景分割

panoptic segmentation





## 分割算法

### 阈值分割

目标和背景占据不同灰度级范围的图

![image-20220308143601490](https://picgo-freejim.oss-cn-beijing.aliyuncs.com/to_upload/image-20220308143601490.png)



## 阈值分割



## 边缘分割

边缘灰度值变化剧烈



## 报错

```
Another metric with the same name already exists.
```

解决

kersa版本不对

```
pip uninstall keras
```

```
pip install keras==2.6.0
```







# openCV



## 阈值

[demo](https://learnopencv.com/opencv-threshold-python-cpp/)



## 灰度值

0是黑色，255是白色



## 阈值

```python
retval, threshold_img = cv2.threshold(gray_img, 120, 255, cv2.THRESH_BINARY_INV)
```

高于120的灰度值统一置为255（白）





## getStructuringElement

函数会返回指定形状和尺寸的结构元素

```python
kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (13, 3))
```



矩形：MORPH_RECT

交叉形：MORPH_CROSS

椭圆形：MORPH_ELLIPSE

第二和第三个参数分别是内核的尺寸以及锚点的位置



```python
>>> import cv2
>>> element = cv2.getStructuringElement(cv2.MORPH_CROSS,(5,5))
>>> element
array([[0, 0, 1, 0, 0],
       [0, 0, 1, 0, 0],
       [1, 1, 1, 1, 1],
       [0, 0, 1, 0, 0],
       [0, 0, 1, 0, 0]], dtype=uint8)
```





## dilate

返回值为胀化后的图像

```python
cropImg = dilate_img[rect[0]:rect[1],0:sp[1]]
```



对图片进行膨胀处理

![image-20220309083517992](https://picgo-freejim.oss-cn-beijing.aliyuncs.com/to_upload/image-20220309083517992.png)

胀化

![image-20220309083532567](https://picgo-freejim.oss-cn-beijing.aliyuncs.com/to_upload/image-20220309083532567.png)









## OCR

[原文地址](https://blog.csdn.net/qq_43555843/article/details/117412056)

代码注释

```python
#小票水平分割
import cv2
import numpy as np

img = cv2.imread(r"img.png")
cv2.imshow("Orig Image", img)

# 输出图像尺寸和通道信息
sp = img.shape
print("图像信息：", sp)

sz1 = sp[0]  # height
sz2 = sp[1]  # width
sz3 = sp[2]  # the pixels value is made up of three primary colors
print('width: %d \n height: %d \n number: %d' % (sz2, sz1, sz3))

gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)

# 转化为gray后
# 有字区域threshold到255 - 亮
retval, threshold_img = cv2.threshold(gray_img, 120, 255, cv2.THRESH_BINARY_INV)
cv2.imshow("threshold_img", threshold_img)

# 水平投影分割图像
gray_value_x = []
for i in range(sz1):
    white_value = 0
    # 是字的白的，统计起来
    # 统计一行过去，为有字区域的像素点数
    for j in range(sz2):
        if threshold_img[i, j] == 255:
            white_value += 1

    gray_value_x.append(white_value)


print(gray_value_x)

# 直方图统计
hori_projection_img = np.zeros((sp[0], sp[1], 1), np.uint8)


# 逐行处理
for i in range(sz1):
    # 长短不一
    for j in range(gray_value_x[i]):
        # j的值从0到gray_value_x[i] - 有字区域的像素点数
        hori_projection_img[i, j] = 255

cv2.imshow("hori_projection_img", hori_projection_img)
text_rect = []

# 根据水平投影分割识别行
inline_x = 0
start_x = 0
text_rect_x = []

# 字
# 如果有字，gray_value_x[i]>10
# inline - 某一行
# start_x 置i - 某一行开始竖向开始了
# 文字 - gray_value_x[i]>10 - inline_x=1, start_x=i

# 背景
# gray_value_x[i]<10 为背景，该行没有字

# 取竖线区间
for i in range(len(gray_value_x)):
    if inline_x == 0 and gray_value_x[i] > 10:
        inline_x = 1
        start_x = i

        # 下次进来的时候如果不再是有字区域了的话，才截取
        # 否则，仍然属于连续的句子，继续延展，不进入这个elif
    elif inline_x == 1 and gray_value_x[i] < 10 and (i - start_x) > 5:
        inline_x = 0
        # 至少一个字的宽度
        if i - start_x > 10:

            # rect是list，表示一区间
            # 竖向划线
            rect = [start_x - 1, i + 1]

            # len(text_rect_x)为总共有几行文字
            text_rect_x.append(rect)

print("分行区域，每行数据起始位置Y：", text_rect_x)


# 获取矩形区域的kernel用于胀化文字部分
# 全1的矩形区域,二维数组
# (13,3)约为字的大小
kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (13, 3))

# 获得胀化后的图像
dilate_img = cv2.dilate(threshold_img, kernel)
cv2.imshow("dilate_img", dilate_img)

for rect in text_rect_x:
    # 矩形的宽
    # 矩形的长 - 0:sp[1] 为一行从左到右
    # 横向裁出一块
    cropImg = dilate_img[rect[0]:rect[1], 0:sp[1]]

    # 一行文字的宽高
    sp_y = cropImg.shape

    # 处理一行矩形的图像
    gray_value_y = []
    for i in range(sp_y[1]):
        white_value = 0
        for j in range(sp_y[0]):

            # 胀化后的图像，统计文字区域
            if cropImg[j, i] == 255:
                white_value += 1
        gray_value_y.append(white_value)

    # 一行文字的宽高
    veri_projection_img = np.zeros((sp_y[0], sp_y[1], 1), np.uint8)

    # 高
    for i in range(sp_y[1]):
        for j in range(gray_value_y[i]):
            veri_projection_img[j, i] = 255
    cv2.imshow("veri_projection_img", veri_projection_img)

    # 取横向x区间
    inline_y = 0
    start_y = 0
    text_rect_y = []
    for i in range(len(gray_value_y)):
        if inline_y == 0 and gray_value_y[i] > 2:
            inline_y = 1
            start_y = i
        elif inline_y == 1 and gray_value_y[i] < 2 and (i - start_y) > 5:
            inline_y = 0
            if i - start_y > 10:
                rect_y = [start_y - 1, i + 1]
                text_rect_y.append(rect_y)

                # 一个矩形的四角坐标
                text_rect.append([rect[0], rect[1], start_y - 1, i + 1])

                # 在threshold图像中截取矩形区域
                cropImg_rect = threshold_img[rect[0]:rect[1], start_y - 1:i + 1]
                cv2.imshow("cropImg_rect", cropImg_rect)


# 在原图上绘制截图矩形区域
print("截取矩形区域(y-start:y-end,x-start:x-end)：", text_rect)
rectangle_img = cv2.rectangle(img, (text_rect[0][2], text_rect[0][0]), (text_rect[0][3], text_rect[0][1]),
                              (255, 0, 0), thickness=1)
# 在原图像中画出矩形框
# rectangle 的参数
# img,startPoint,EndPoint,color,thickness
# startPoint是左上角，endPoint是右下角
# startPoint(100,50)，表示往右x=100,往下y=50
# text_rect (start_y,end_y,start_x,end_x)
# 所以下面为(2,0) (3,1)解析为:(start_x,start_y) (end_x,end_y)
for rect_roi in text_rect:
    rectangle_img = cv2.rectangle(img, (rect_roi[2], rect_roi[0]), (rect_roi[3], rect_roi[1]), (255, 0, 0), thickness=1)
cv2.imshow("Rectangle Image", rectangle_img)

key = cv2.waitKey(0)
if key == 27:
    print(key)
    cv2.destroyAllWindows()

```



### 转黑白

```python
img = cv2.imread(r"img.png")
gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
```

![image-20220309113900247](https://picgo-freejim.oss-cn-beijing.aliyuncs.com/to_upload/image-20220309113900247.png)





### 统计高亮文字

如果当前行切到了字，那么`white_value` 的值便不会小

```python
gray_value_x = []
for i in range(sz1):
    white_value = 0
    # 统计一行过去，为背景像素点的个数
    for j in range(sz2):
        if threshold_img[i, j] == 255:
            white_value += 1

    gray_value_x.append(white_value)
```

Ps：原图找不到了，下图是已经画上矩形的图片

![image-20220309113746078](https://picgo-freejim.oss-cn-beijing.aliyuncs.com/to_upload/image-20220309113746078.png)





### 胀化

Ps：原图找不到了，下图是已经画上矩形的图片

![image-20220309114014224](https://picgo-freejim.oss-cn-beijing.aliyuncs.com/to_upload/image-20220309114014224.png)







![](https://picgo-freejim.oss-cn-beijing.aliyuncs.com/to_upload/image-20220309092842081.png)





## 盲文分割

### 分割单元

有些盲文字为2个单元，有些为3个单元

如何处理

```python
# size(100,100)
# 一点高15
# 两盲文文字约为70
if i - start_y > 70:
    print('画横线')
```



那就每次分割一个盲文字单元

如果识别到声调

那么就合并为一个字

得到一个大的矩形区域

再进行识别





### 盲文点的threshold

```python
retval, threshold_img = cv2.threshold(gray_img, 160, 255, cv2.THRESH_BINARY_INV)
```

盲文点突起，有高亮









### 盲文数字

一个单位一个单位把数字拼接上





## 报错

```
(-215:Assertion failed) !_src.empty() in function 'cv::cvtColor'
```

一般是路径有问题，导致img为空













# Python

## 数据集加载



## python多分隔符

```python
# 用|分隔多个分隔符
re.split('\t|    |		|   ', text)
```



## python查看数组

```python
print(labels[0:20])
```





## ndarray

### 初始化

```python
images = np.array()
```



### 增加元素

```python
images = np.append(images,img)
```



### 区间

```python

```



## shape

```python

```







## MNIST

手写数字的图片和相应的标签组成，图片一共有10 类，分别对应从0～9 ，共10 个阿拉伯数字

![image-20220305194308792](https://picgo-freejim.oss-cn-beijing.aliyuncs.com/to_upload/image-20220305194308792.png)







## 文件操作

### 获取文件列表

```python
list_dir = os.listdir(PATH)
```



## whl安装库

```
pip install xx.whl
```



## pip镜像

```
pip install xxx -i https://mirrors.ustc.edu.cn/pypi/web/simple/
```



 ## 类

错误

```
from src.utils import TrainModel
```

正确

```
from src.utils.TrainModel import TrainModel
```







# Pycharm

## import xxx can not resolve 

官方文档

```
from tensorflow.keras import datasets, layers, models
```

实际手打，发现需要：

```
from tensorflow.python.keras import layers, models
```





# git

## git add添加指定文件





## git 撤销add





# Mysql

## mysql插入中文乱码

```
spring.datasource.url=jdbc:mysql://localhost:3306/smdb?serverTimezone=UTC&useUnicode=true&characterEncoding=UTF-8
```





# 文字转语音

## 微软平台

[网站](https://portal.azure.cn/#home)

```
# 微软账号
- https://portal.partner.microsoftonline.cn/
- culiu@digimicro.partner.onmschina.cn
- e8EcvSMSNsu!C$s
```



[github仓库](https://github.com/Azure-Samples/cognitive-services-speech-sdk)





# 问题



## python项目导出requirement.txt



## 腐蚀+胀化并不是对所有图像都有效



## python 工作目录

```python
os.path.abspath('..')
```





## 分割行高





# API

```
http://47.94.138.85:8080/upload
```







